<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>CPU Matrix Multiplication from Scratch in C &#8212; Pebbles and Weeds Blog 1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=c058f7c8" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=ec641305" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=a43d319b" />
    <script src="_static/documentation_options.js?v=f2a433a1"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://blog.pebblesandweeds.com/cpu_matmul_blog.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Accelerating Matrix Multiplication on AMD GPUs with rocBLAS in C" href="gpu_matmul_blog.html" />
    <link rel="prev" title="Pebbles and Weeds Blog" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
          

          <div class="body" role="main">
            
  <section id="cpu-matrix-multiplication-from-scratch-in-c">
<span id="matrix-multiplication"></span><h1>CPU Matrix Multiplication from Scratch in C<a class="headerlink" href="#cpu-matrix-multiplication-from-scratch-in-c" title="Link to this heading">¶</a></h1>
<div class="admonition-highlights admonition">
<p class="admonition-title">Highlights</p>
<p>Matrix multiplication is fundamental in deep learning, acting as the primary mechanism for executiong computations in neural networks. This blog post explores matrix multiplication using C to achieve performance comparable to Python’s NumPy and explores detailed optimizations that leverage C’s low-level capabilities.</p>
<p>We demonstrate the performance evolution on an <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/c7a/">AWS c7a.32xlarge EC2 instance</a> through the following implementations using 32-bit precision:</p>
<ul class="simple">
<li><p><strong>Baseline with Python/NumPy</strong>: <strong>~3,500 GFLOPS</strong>, using <a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul/blob/main/python/numpy_matmul.py">this implementation</a> for comparison with our C code.</p></li>
<li><p><strong>Naive C Approach</strong>: <strong>~25 GFLOPS</strong>, starting with a simple <a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul/blob/main/c/src/matmul_lib.c#L28">scalar implementation</a>.</p></li>
<li><p><strong>Optimized Scalar C</strong>: <strong>~500 GFLOPS</strong>, optimizing the <a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul/blob/main/c/src/matmul_lib.c#L39">naive scalar implementation</a> using tiling, blocking, and loop unrolling.</p></li>
<li><p><strong>Vectorized C Operations</strong>: <strong>~3,000 GFLOPS</strong>, leveraging <a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul/blob/main/c/src/matmul_lib.c#L64">vectorized instructions</a> for enhanced performance.</p></li>
</ul>
<p>These results demonstrate the effectiveness of implementing matrix multiplication from scratch in C, achieving performance levels close to those of Python’s NumPy.</p>
<p>Get all of the code <a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul">in this repo</a>.</p>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">¶</a></h2>
<p>Matrix multiplication is a core computational operation in machine learning, particularly in deep learning and neural networks.  Multiplying matrices is important for translating and scaling input data across various computational structures, a key step in processing interactions of data features within models architectures.  Optimizing computational efficiency and accuracy during both model training and inference phases directly relies on fast matrix multiplication.</p>
<p>This blog explores building C implementations of CPU matrix multiplication from scratch, focusing on efficient algorithms and low-level optimizations. By working with C, we gain insights into performance enhancements often hidden by high-level libraries, critical for scaling matrix multiplication calculations for transformer based models to handle large volumes of computations.</p>
</section>
<section id="why-is-matrix-multiplication-important">
<h2>Why is Matrix Multiplication Important?<a class="headerlink" href="#why-is-matrix-multiplication-important" title="Link to this heading">¶</a></h2>
<p>Matrix multiplication is essential to neural network computations, particularly in forward and backward propagation.</p>
<ul class="simple">
<li><p><strong>Forward Propagation:</strong> Matrix multiplication combines input data with learned weights, producing activations that flow through the network.</p></li>
<li><p><strong>Backward Propagation:</strong> Matrix multiplication helps calculate gradients, enabling the network to update its weights and learn from errors.</p></li>
</ul>
<p>The efficiency of matrix multiplication directly impacts the speed and scalability of neural network training and inference.</p>
<p>Matrix multiplication is vital in many machine learning domains, including natural language processing, computer vision, and audio processing. It is central to transforming input data, computing activations, and updating model parameters. These domains heavily rely on transformer architectures, where matrix multiplication powers self-attention mechanisms and feedforward networks, enabling the efficient computation of attention weights and data transformation through network layers.</p>
<p>As model sizes grow, computational demands increase significantly. Though no longer at the cutting edge, architectures like GPT-2 and GPT-3 still involve millions of matrix multiplications across multiple layers, attention mechanisms, embedding layers, position-wise feedforward networks, and output linear layers, highlighting the significant computational load requred to process and propagate inputs, activations, and gradients through these complex networks.</p>
<p>Optimizing matrix multiplication is therefore critical for improving the efficiency of large-scale models. It directly affects training time, resource consumption, and the ability to scale to larger datasets and model sizes. Understanding and optimizing this operation is key to advancing the capabilities of machine learning models while managing computational resources effectively.</p>
</section>
<section id="matrix-multiplication-on-cpus-written-in-c">
<h2>Matrix Multiplication on CPUs Written in C<a class="headerlink" href="#matrix-multiplication-on-cpus-written-in-c" title="Link to this heading">¶</a></h2>
<p><strong>Why aren’t we using GPUs?</strong></p>
<ul class="simple">
<li><p>Starting with CPUs helps establish a foundational understanding of matrix multiplication and optimization techniques.</p></li>
<li><p>Implementing matrix operations on CPUs helps us better understand performance optimization challenges and prepares us for future GPU implementations.</p></li>
</ul>
<p><strong>Why aren’t we using Python?</strong></p>
<ul class="simple">
<li><p>C provides low-level access to system resources and control over execution, ideal for optimizing performance in computationally intensive tasks.</p></li>
<li><p>Building an implementation of matrix multiplication in C improves understanding of the memory hierarchy and optimizes access patterns, which are important for high-performance matrix operations.</p></li>
<li><p>We can compare our C implementation to Python’s NumPy to benchmark performance to enhance our understanding of matrix multiplication and optimization techniques.</p></li>
</ul>
</section>
<section id="benchmarking-setup-and-code-organization">
<h2>Benchmarking Setup and Code Organization<a class="headerlink" href="#benchmarking-setup-and-code-organization" title="Link to this heading">¶</a></h2>
<section id="matrix-configuration-and-benchmarking-strategy">
<h3><em>Matrix Configuration and Benchmarking Strategy</em><a class="headerlink" href="#matrix-configuration-and-benchmarking-strategy" title="Link to this heading">¶</a></h3>
<p>Our implementation performs matrix multiplication using the formula C = A x B, where both matrices A and B are square matrices of size N × N. We set N to a static size of 8,192, simplifying the implementation and laying the groundwork for future extensions to non-square matrices. By defining N with a preprocessor C macro (<code class="docutils literal notranslate"><span class="pre">#define</span> <span class="pre">N</span> <span class="pre">8192</span></code>), we can enable aggressive compiler optimizations and ensure consistent runtime behavior.</p>
<p>In this setup, we are not implementing separate kernels with varying block sizes because each matrix is fixed at N × N. A kernel typically refers to an optimized code block designed for flexible execution across varying data sizes or hardware conditions. Since N is static in our implementation, the complexity of multiple kernels is unnecessary, allowing us to focus on optimizing for a single, fixed configuration.</p>
<section id="memory-requirements">
<h4>Memory Requirements<a class="headerlink" href="#memory-requirements" title="Link to this heading">¶</a></h4>
<p>With N = 8,192, each matrix contains 67,108,864 elements. Using 32-bit floating-point precision (often referred to as “single precision” or “FP32”), the size of each matrix (A, B, and C) is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[67,108,864 \times 4 \text{ bytes} = 268,435,456 \text{ bytes} \approx 268 \text{ MB}\]</div>
<p>This results in a total memory requirement of approximately 805 MB for all three matrices.</p>
</section>
<section id="computational-complexity">
<h4>Computational Complexity<a class="headerlink" href="#computational-complexity" title="Link to this heading">¶</a></h4>
<p>Calculating the computational effort for matrix multiplication involves determining the total number of floating point operations (FLOPs) needed. When multiplying two <span class="math notranslate nohighlight">\(N \times N\)</span> matrices, the resulting matrix is also <span class="math notranslate nohighlight">\(N \times N\)</span> (<span class="math notranslate nohighlight">\(N^2\)</span> elements). Each element is the result of a dot product between a row from the first matrix and a column from the second matrix. This involves:</p>
<ul class="simple">
<li><p><strong>Multiplications:</strong> Each element requires multiplying <span class="math notranslate nohighlight">\(N\)</span> pairs of numbers (one from the row and one from the column).</p></li>
<li><p><strong>Additions:</strong> The products from the multiplications are then summed together, requiring <span class="math notranslate nohighlight">\(N - 1\)</span> additions (adding two numbers requires one addition, adding three numbers requires two additions, etc).</p></li>
</ul>
<p>Thus, the total number of FLOPs is calculated as:</p>
<div class="math notranslate nohighlight">
\[\text{Total FLOPs} = 2N^3 - N^2\]</div>
<p>For large matrices, the <span class="math notranslate nohighlight">\(2N^3\)</span> term contributes primarily to the total FLOPs, so it is often used to estimate the computational effort. This simplifies to:</p>
<div class="math notranslate nohighlight">
\[\text{Total FLOPs} = 2N^3\]</div>
<p>This simplification highlights how the computational effort grows with the size of the matrices. For our chosen matrix size of 8192 x 8192, this results in:</p>
<div class="math notranslate nohighlight">
\[2 \times 8192^3 = 1,099,511,627,776 \approx 1.1 \text{ TFLOPs}\]</div>
<p>This large number of operations underscores the computational intensity of large-scale matrix multiplication and highlights the importance of our optimization efforts. It is also important to note the distinction between FLOPs, which measure the total operations required, and FLOPS (Floating Point Operations Per Second), which indicate the system’s performance capability.</p>
</section>
<section id="cache-considerations">
<h4>Cache Considerations<a class="headerlink" href="#cache-considerations" title="Link to this heading">¶</a></h4>
<p>We chose this large N value (8,192) to represent a realistic problem size for our matrix multiplication.  With our matrix size of approximately 268MB each, the entire problem (all three matrices) doesn’t fit in L3 cache simultaneously, but significant portions of the working set can potentially reside in cache during computation. This creates a scenario where careful cache management becomes crucial for performance. Our setup allows us to:</p>
<ul class="simple">
<li><p>Explore the effects of cache blocking and tiling optimizations</p></li>
<li><p>Observe how different algorithms balance cache utilization and main memory access</p></li>
<li><p>Understand performance characteristics that bridge cached and non-cached operations</p></li>
<li><p>Investigate how implementations handle a problem that doesn’t neatly fit entirely in cache, but is also not so large as to make cache optimizations irrelevant</p></li>
</ul>
<p>This approach provides insight into algorithm design for real-world, cache-sensitive computations.</p>
</section>
<section id="benchmarking-environment">
<h4>Benchmarking Environment<a class="headerlink" href="#benchmarking-environment" title="Link to this heading">¶</a></h4>
<p>For our benchmarks, we used an AWS c7a.32xlarge instance with the following specifications:</p>
<ul class="simple">
<li><p><strong>Processor:</strong> AMD EPYC 9R14</p></li>
<li><p><strong>Cores:</strong> 2 sockets, 64 cores per socket (128 cores total, without simultaneous multithreading)</p></li>
<li><p><strong>L3 Cache:</strong> 512MB</p></li>
</ul>
<p>The total working set size is about 805MB (three 268MB matrices), which is larger than the L3 cache. This setup allows us to observe how the cache handles large matrix multiplications and its impact on performance, as the entire workload cannot fit in the cache at once.  This setup ensures the dataset exceeds the cache size, providing a realistic assessment of the algorithm’s performance.</p>
</section>
</section>
<section id="code-structure-and-organization">
<h3><em>Code Structure and Organization</em><a class="headerlink" href="#code-structure-and-organization" title="Link to this heading">¶</a></h3>
<p>Our matrix multiplication code is organized into separate modules for clarity and maintainability. The primary files are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul/blob/dev/c/src/matmul_lib.c">matmul_lib.c</a>: Contains the core matrix multiplication functions.</p></li>
<li><p><a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul/blob/dev/c/src/main.c">main.c</a>: Serves as the entry point, calling functions from <code class="docutils literal notranslate"><span class="pre">matmul_lib.c</span></code>.</p></li>
<li><p><a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul/blob/main/c/Makefile">Makefile</a>: Specifies the build process using the <code class="docutils literal notranslate"><span class="pre">gcc</span></code> compiler with optimization flags <code class="docutils literal notranslate"><span class="pre">CFLAGS</span> <span class="pre">=</span> <span class="pre">-mavx2</span> <span class="pre">-fopenmp</span> <span class="pre">-O3</span> <span class="pre">-march=native</span> <span class="pre">-I./include</span></code></p></li>
</ul>
<p>For a detailed overview of our project structure and how we implement various matrix multiplication methods and optimizations, refer to our <a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul/blob/dev/README.md#project-structure">README.md</a>. The code snippets in this blog exclude <cite>#pragma</cite> directives for simplicity; the full code with parallel instructions is available in the repository.</p>
</section>
</section>
<section id="naive-matrix-multiplication">
<h2>Naive Matrix Multiplication<a class="headerlink" href="#naive-matrix-multiplication" title="Link to this heading">¶</a></h2>
<p>We begin with a basic matrix multiplication method in C to illustrate the fundamental algorithm and its inefficiencies. The following sections will provide a visual representation, the mathematical formula, and the implementation of this approach.</p>
<section id="visual-and-formulaic-representation">
<h3><em>Visual and Formulaic Representation</em><a class="headerlink" href="#visual-and-formulaic-representation" title="Link to this heading">¶</a></h3>
<p>The process is illustrated with an animation showing an 8x8 matrix multiplication. Each frame captures the computation of matrix <span class="math notranslate nohighlight">\(C\)</span> elements as the sum of products from matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<img alt="8x8 Matrix Multiplication Animation" class="align-center" src="_images/matrix_multiplication_8x8_precise_loop.gif" />
<p>The corresponding mathematical operation is described by the formula:</p>
<div class="math notranslate nohighlight">
\[C_{ij} = \sum_{k=1}^{N} A_{ik} B_{kj}\]</div>
</section>
<section id="naive-implementation-in-c">
<h3><em>Naive Implementation in C</em><a class="headerlink" href="#naive-implementation-in-c" title="Link to this heading">¶</a></h3>
<p>Following this formula, our C code implementation employs three nested loops to perform the matrix multiplication. This basic method is straightforward but not optimized for performance, particularly with large matrices where the computational overhead becomes significant.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">matmul</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="naive-matrix-multiplication-performance">
<h3><em>Naive Matrix Multiplication Performance</em><a class="headerlink" href="#naive-matrix-multiplication-performance" title="Link to this heading">¶</a></h3>
<p>This naive approach effectively illustrates the link between algorithmic simplicity and computational inefficiency. With N set to 8,192, the computation involves approximately 1,099.51 billion floating-point operations. Despite the high-end CPU, our AWS c7a.32xlarge instance only achieves a performance of <strong>~25 GFLOPS</strong>.  This demonstrates the significant gap between the naive method’s performance and the optimizations needed and sets the stage for exploring more advanced optimization techniques in the following sections.</p>
</section>
</section>
<section id="optimizing-matrix-multiplication">
<h2>Optimizing Matrix Multiplication<a class="headerlink" href="#optimizing-matrix-multiplication" title="Link to this heading">¶</a></h2>
<p>While the naive matrix multiplication implementation helps understand the basic algorithm, it is inefficient for large matrices.  It processes matrices in row-major order, the default in C, where rows of matrix A are multiplied by columns of matrix B. This access pattern leads to frequent cache misses because it disrupts spatial locality, as matrix elements are stored contiguously in memory. The mismatch between access patterns and memory layout results in poor cache utilization and increased memory latency, significantly impacting performance.</p>
<p>To address these inefficiencies, we use tiling, blocking, and loop unrolling. Tiling and blocking restructure computations to improve data locality by dividing matrices into smaller blocks, which enhances cache usage. Loop unrolling reduces the overhead of loop control by expanding loops, allowing more operations to be performed in parallel. These methods collectively improve data locality and make better use of CPU caches, significantly enhancing performance. For more detailed information on these techniques, see <a class="reference external" href="https://en.wikipedia.org/wiki/Loop_nest_optimization#Tiling">Tiling and Blocking</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Loop_unrolling">Loop Unrolling</a>.</p>
<section id="optimized-implementation-in-c">
<h3><em>Optimized Implementation in C</em><a class="headerlink" href="#optimized-implementation-in-c" title="Link to this heading">¶</a></h3>
<p>Our optimized matrix multiplication implementation leverages these techniques to minimize cache misses and maximize computational throughput. The following C code demonstrates the use of blocking, tiling, and unrolling to improve performance:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#define BLOCK_SIZE 64 </span><span class="c1">// Optimizes memory across L1/L2/L3; fetch data in chunks</span>
<span class="cp">#define TILE_SIZE 32 </span><span class="c1">// Improves CPUs data processing; balances CPU resources and data caching</span>
<span class="cp">#define UNROLL_FACTOR 4 </span><span class="c1">// Increases parallel operations w/out overwhelming memory</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">matmul_scalar</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span><span class="w"> </span><span class="p">{</span>
<span class="c1">// Outer loops for block-wise operations</span>
<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="c1">// Inner loops for tile-wise operations within blocks</span>
<span class="w">     </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">TILE_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">j</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">TILE_SIZE</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="c1">// Loop unrolling for innermost loop</span>
<span class="w">     </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">kk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">k</span><span class="p">;</span><span class="w"> </span><span class="n">kk</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">kk</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">kk</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">UNROLL_FACTOR</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="kt">float</span><span class="w"> </span><span class="n">c_temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">];</span><span class="w"> </span><span class="c1">// Temp variable for accumulation</span>
<span class="w">         </span><span class="c1">// Compute on tiles</span>
<span class="w">         </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">iii</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ii</span><span class="p">;</span><span class="w"> </span><span class="n">iii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">TILE_SIZE</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">iii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">iii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">iii</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">jjj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jj</span><span class="p">;</span><span class="w"> </span><span class="n">jjj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">TILE_SIZE</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">jjj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">BLOCK_SIZE</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">jjj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">jjj</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">             </span><span class="c1">// Matrix multiplication within a tile</span>
<span class="w">             </span><span class="n">c_temp</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">iii</span><span class="p">][</span><span class="n">kk</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">kk</span><span class="p">][</span><span class="n">jjj</span><span class="p">];</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">         </span><span class="n">C</span><span class="p">[</span><span class="n">iii</span><span class="p">][</span><span class="n">jjj</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c_temp</span><span class="p">;</span><span class="w"> </span><span class="c1">// Store accumulated results</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">     </span><span class="p">}</span>
<span class="w">     </span><span class="p">}</span>
<span class="w">     </span><span class="p">}</span>
<span class="w"> </span><span class="p">}</span>
<span class="w"> </span><span class="p">}</span>
<span class="w"> </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="optimized-matrix-multiplication-performance">
<h3><em>Optimized Matrix Multiplication Performance</em><a class="headerlink" href="#optimized-matrix-multiplication-performance" title="Link to this heading">¶</a></h3>
<p>By optimizing matrix multiplication, we achieve a significant performance boost. Our approach in the code above employs three key strategies: dividing matrices into cache-friendly blocks, further subdividing into efficiently processable tiles, and using loop unrolling for parallel operations. These techniques work together to ensure optimal data availability and CPU resource utilization.</p>
<p>On the AWS c7a.32xlarge instance, this optimized implementation achieves approximately <strong>500 GFLOPS</strong>, representing more than a <em>20x increase</em> over the naive approach. This improvement stems from better use of the CPU’s cache hierarchy, reduced memory access times, and increased instruction-level parallelism. While further scalar optimizations are possible, we’re approaching the limits of what can be achieved without leveraging more advanced hardware features. The next step in boosting performance is to utilize vectorized operations, which we’ll explore in the following section.</p>
</section>
</section>
<section id="vectorized-matrix-multiplication">
<h2>Vectorized Matrix Multiplication<a class="headerlink" href="#vectorized-matrix-multiplication" title="Link to this heading">¶</a></h2>
<section id="scalar-vs-vectorized-operations">
<h3><em>Scalar vs. Vectorized Operations</em><a class="headerlink" href="#scalar-vs-vectorized-operations" title="Link to this heading">¶</a></h3>
<p>Scalar operations process data one element at a time, performing calculations sequentially. In contrast, vectorized operations use a Single Instruction, Multiple Data (SIMD) approach, processing multiple data elements simultaneously. This parallelism is implemented on CPUs through SIMD instructions, which leverage hardware capabilities to execute the same operation on multiple data points in a single instruction cycle.</p>
<p>To write vectorized code, several elements are necessary:</p>
<ol class="arabic simple">
<li><p><strong>SIMD Instructions</strong>: SIMD instructions, such as AVX, enable parallel processing by applying the same operation across multiple data elements in a single instruction. This includes <a class="reference external" href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation">Fused Multiply-Add (FMA)</a>, which performs multiplication and addition together. For more information on SIMD, see <a class="reference external" href="https://en.wikipedia.org/wiki/SIMD">Wikipedia</a>.</p></li>
<li><p><strong>Data Alignment</strong>: Properly aligning data in memory is crucial for SIMD processing. Aligned data ensures that SIMD instructions can access data efficiently, avoiding costly misaligned memory accesses. Learn more about <a class="reference external" href="https://en.wikipedia.org/wiki/Data_structure_alignment">Data Alignment</a>.</p></li>
<li><p><strong>Loop Unrolling</strong>: Loop unrolling enhances vectorized operations by expanding loop iterations, reducing overhead, and allowing more operations to be performed in parallel. This technique improves the efficiency of SIMD instructions. More details can be found at <a class="reference external" href="https://en.wikipedia.org/wiki/Loop_unrolling">Loop Unrolling</a>.</p></li>
<li><p><strong>Prefetching</strong>: Prefetching involves loading data into the CPU cache before it is needed, reducing cache misses and ensuring that data is readily available when required. This technique optimizes memory access patterns and improves performance. Learn about <a class="reference external" href="https://en.wikipedia.org/wiki/Cache_prefetching">Prefetching</a>.</p></li>
<li><p><strong>Transposition</strong>: Matrix transposition rearranges data to improve access patterns, particularly for matrix operations. By aligning data in a more efficient layout, transposition reduces cache misses and speeds up computations. For more on this, see <a class="reference external" href="https://en.wikipedia.org/wiki/Transpose">Matrix Transposition</a>.</p></li>
</ol>
</section>
<section id="vectorized-implementation-in-c">
<h3><em>Vectorized Implementation in C</em><a class="headerlink" href="#vectorized-implementation-in-c" title="Link to this heading">¶</a></h3>
<p>Below is the C implementation of matrix multiplication using vectorization techniques to enhance performance:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">matmul_vectorized</span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Data alignment (allocate memory for B_col)</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">B_col</span><span class="p">)[</span><span class="n">N</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">aligned_alloc</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">B_col</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Memory allocation failed</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// Transposition (transpose B into B_col for better memory access patterns)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">B_col</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="n">jj</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="o">+</span><span class="n">jj</span><span class="p">];</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="c1">// SIMD instructions (__m256 for 256-bit for SIMD operations)</span>
<span class="w">                </span><span class="n">__m256</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">32</span><span class="p">];</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_setzero_ps</span><span class="p">();</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="c1">// Prefetching (fetch data into cache before we use it)</span>
<span class="w">                    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">128</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                            </span><span class="n">_mm_prefetch</span><span class="p">((</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">ii</span><span class="p">][</span><span class="n">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="n">_MM_HINT_T1</span><span class="p">);</span>
<span class="w">                            </span><span class="n">_mm_prefetch</span><span class="p">((</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">B_col</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="n">ii</span><span class="p">][</span><span class="n">k</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="n">_MM_HINT_T1</span><span class="p">);</span>
<span class="w">                        </span><span class="p">}</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                    </span><span class="n">__m256</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">4</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="mi">32</span><span class="p">][</span><span class="mi">4</span><span class="p">];</span>
<span class="w">                    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">kk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">kk</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span><span class="w"> </span><span class="n">kk</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                            </span><span class="n">a</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">kk</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_loadu_ps</span><span class="p">(</span><span class="o">&amp;</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">ii</span><span class="p">][</span><span class="n">k</span><span class="o">+</span><span class="n">kk</span><span class="o">*</span><span class="mi">8</span><span class="p">]);</span>
<span class="w">                            </span><span class="n">b</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">kk</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_load_ps</span><span class="p">(</span><span class="o">&amp;</span><span class="n">B_col</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="n">ii</span><span class="p">][</span><span class="n">k</span><span class="o">+</span><span class="n">kk</span><span class="o">*</span><span class="mi">8</span><span class="p">]);</span>
<span class="w">                        </span><span class="p">}</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                    </span><span class="c1">// Loop unrolling (unroll inner loop for vector operations) and FMA (fused multiply-add)</span>
<span class="w">                    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                            </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_fmadd_ps</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">jj</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]);</span>
<span class="w">                            </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_fmadd_ps</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">jj</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]);</span>
<span class="w">                            </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_fmadd_ps</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">jj</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]);</span>
<span class="w">                            </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_fmadd_ps</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="mi">3</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">jj</span><span class="p">][</span><span class="mi">3</span><span class="p">],</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]);</span>
<span class="w">                        </span><span class="p">}</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">                </span><span class="c1">// SIMD Instructions (final matrix multiplication reduction using SIMD)</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ii</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">ii</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">32</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">jj</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">jj</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                        </span><span class="n">__m256</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">];</span>
<span class="w">                        </span><span class="kr">__m128</span><span class="w"> </span><span class="n">sum_high</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_extractf128_ps</span><span class="p">(</span><span class="n">sum</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">                        </span><span class="kr">__m128</span><span class="w"> </span><span class="n">sum_low</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm256_castps256_ps128</span><span class="p">(</span><span class="n">sum</span><span class="p">);</span>
<span class="w">                        </span><span class="kr">__m128</span><span class="w"> </span><span class="n">sum_all</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm_add_ps</span><span class="p">(</span><span class="n">sum_high</span><span class="p">,</span><span class="w"> </span><span class="n">sum_low</span><span class="p">);</span>
<span class="w">                        </span><span class="n">sum_all</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm_hadd_ps</span><span class="p">(</span><span class="n">sum_all</span><span class="p">,</span><span class="w"> </span><span class="n">sum_all</span><span class="p">);</span>
<span class="w">                        </span><span class="n">sum_all</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm_hadd_ps</span><span class="p">(</span><span class="n">sum_all</span><span class="p">,</span><span class="w"> </span><span class="n">sum_all</span><span class="p">);</span>
<span class="w">                        </span><span class="kt">float</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm_cvtss_f32</span><span class="p">(</span><span class="n">sum_all</span><span class="p">);</span>
<span class="w">                        </span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">ii</span><span class="p">][</span><span class="n">j</span><span class="o">+</span><span class="n">jj</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">result</span><span class="p">;</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">free</span><span class="p">(</span><span class="n">B_col</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="performance-improvement">
<h3><em>Performance Improvement</em><a class="headerlink" href="#performance-improvement" title="Link to this heading">¶</a></h3>
<p>The vectorized implementation greatly improves performance by applying the vectorized techniques described earlier. Data alignment optimizes memory access for SIMD operations, while transposition refines data layout to enhance access patterns for matrix operations. SIMD instructions and 256-bit AVX <a class="reference external" href="https://en.wikipedia.org/wiki/Processor_register">YMM registers</a> enable parallel processing of up to eight single-precision floating-point numbers per cycle, boosting data throughput. Prefetching reduces cache misses by pre-loading data, and loop unrolling enhances vector operation efficiency by cutting loop overhead and allowing more parallel instruction execution. These combined techniques leverage the CPU’s vectorization capabilities to deliver substantial performance gains.</p>
<p>On the AWS c7a.32xlarge instance, this vectorized approach achieves approximately <strong>3,000 GFLOPS</strong>, representing a <em>6x performance increase</em> over the previously optimized scalar implementation.  This contrast underscores the efficiency of vectorized operations, which use SIMD to process multiple data elements simultaneously along with our other alighment optimizations.  This significant performance gain highlights the effectiveness of these advanced techniques in enhancing computational efficiency for large-scale matrix operations.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">¶</a></h2>
<p>Our exploration of matrix multiplication optimization reveals significant performance gains. Starting with a naive implementation at 25 GFLOPS, we improved to 500 GFLOPS with scalar optimization, marking a 20x increase. Vectorized operations then further boosted performance to 3,000 GFLOPS, achieving a 120x improvement from the initial implementation. This progress highlights the impact of optimizations such as cache-friendly blocking, efficient tiling, and SIMD vectorization.</p>
<p>Our vectorized C implementation nearly matches NumPy’s 3,500 GFLOPS, showing the effectiveness of low-level optimizations. This experience with CPU optimizations enhances our understanding of memory management and parallelism, providing a strong foundation for future GPU optimizations, where similar principles will be applied in a different context.</p>
<p>Thanks for reading, more details can be our <a class="reference external" href="https://github.com/pebblesandweeds/cpu_matmul">cpu_matmul</a> Github repo. Stay tuned for our next blog, where we will explore matrix multiplication optimizations on GPUs.</p>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/flame/how-to-optimize-gemm">GEMM Optimization Tutorial</a> and <a class="reference external" href="https://github.com/flame/blislab/blob/master/tutorial.pdf">BLISlab Tutorial</a></p></li>
<li><p><a class="reference external" href="https://salykova.github.io/matmul-cpu">Beating NumPy in 150 lines of C Code</a> plus the <a class="reference external" href="https://github.com/salykova/matmul.c">repo</a></p></li>
<li><p>George Hotz’s six hour video stream <a class="reference external" href="https://youtu.be/VgSQ1GOC86s?si=HP1VB1UDF384_xQt">Can You Mutliply a Matrix?</a> and <a class="reference external" href="https://github.com/tinygrad/tinygrad/blob/master/extra/gemm/gemm.c">gemm.c code</a></p></li>
</ul>
</section>
</section>


          </div>
          
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Pebbles and Weeds.
      
      |
      <a href="_sources/cpu_matmul_blog.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>